{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KvGIxp4eyALi"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slippery Nature of the Frozen Lake\n",
    "\n",
    "In the Frozen Lake environment, the parameter `is_slippery` determines whether the agent moves in the intended direction with certainty or if there's a chance of slipping.\n",
    "\n",
    "- When `is_slippery` is **True**, the agent may not always move in the intended direction due to the slippery nature of the frozen lake. In this case, there is a probability of 1/3 that the agent will move in the intended direction, while there is an equal probability of 1/3 for the agent to move in either of the perpendicular directions.\n",
    "\n",
    "- When `is_slippery` is **False**, the agent moves in the intended direction with certainty, without any possibility of slipping.\n",
    "\n",
    "This parameter allows for different levels of stochasticity in the agent's movement, affecting the difficulty and realism of the environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C8HXEZGtyIQh"
   },
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1',is_slippery=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QAoTteifyI2c"
   },
   "outputs": [],
   "source": [
    "def calculate_state_value(env, current_state, value_matrix, discount_factor):\n",
    "\n",
    "    num_actions = env.action_space.n\n",
    "    action_values = np.zeros(shape=num_actions)\n",
    "    for action in range(num_actions):\n",
    "        for transition_prob, next_state, reward, done in env.env.P[current_state][action]:\n",
    "            action_values[action] += transition_prob * (reward + discount_factor * value_matrix[next_state])\n",
    "    return action_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Evaluation Function and Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy Evaluation Function\n",
    "\n",
    "The function iterates through the policy evaluation process until either convergence or the maximum number of iterations is reached.\n",
    "Within each iteration, it updates the state values using the Bellman equation for state values, considering the current policy.\n",
    "Convergence is determined by comparing the maximum change in state values (delta) against the convergence threshold.\n",
    "If the convergence threshold is met, the function terminates and returns the estimated state values.\n",
    "The number of iterations taken for policy evaluation is not printed in this version for brevity.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "\n",
    "\n",
    "policy_matrix: The policy matrix specifying the probability of taking each action in each state.\n",
    "    \n",
    "environment: The Frozen Lake environment.\n",
    "    \n",
    "discount_factor: (Optional) The discount factor used to discount future rewards. Default is 1.0.\n",
    "    \n",
    "convergence_threshold: (Optional) The threshold for convergence, indicating when to stop the evaluation process. Default is 1e-9.\n",
    "    \n",
    "max_iterations: (Optional) The maximum number of iterations allowed for the evaluation. Default is 1000.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EheuMWhxyJd0"
   },
   "outputs": [],
   "source": [
    "def evaluate_policy(policy_matrix, environment, discount_factor=1.0, convergence_threshold=1e-9, max_iterations=1000):\n",
    "    num_states = environment.observation_space.n\n",
    "    evaluation_iterations = 1\n",
    "    state_values = np.zeros(shape=num_states)\n",
    "\n",
    "    for iteration in range(int(max_iterations)):\n",
    "        delta = 0\n",
    "        for current_state in range(num_states):\n",
    "            new_state_value = 0\n",
    "            for action, action_probability in enumerate(policy_matrix[current_state]):\n",
    "                for state_probability, next_state, reward, done in environment.P[current_state][action]:\n",
    "                    new_state_value += action_probability * state_probability * (reward + discount_factor * state_values[next_state])\n",
    "            delta = max(delta, np.abs(state_values[current_state] - new_state_value))\n",
    "            state_values[current_state] = new_state_value\n",
    "\n",
    "        evaluation_iterations += 1\n",
    "\n",
    "        if delta < convergence_threshold:\n",
    "            print(f'Policy evaluation terminated after {evaluation_iterations} iterations.\\n')\n",
    "            return state_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function performs policy iteration by iteratively evaluating and improving the policy until convergence or the maximum number of iterations is reached.\n",
    "It initializes the policy matrix with a uniform distribution over actions for each state.\n",
    "In each iteration:\n",
    "It evaluates the current policy using the evaluate_policy function.\n",
    "It then improves the policy by selecting the action that maximizes the expected value for each state.\n",
    "If the policy remains unchanged after an iteration, indicating convergence, the algorithm terminates.\n",
    "Finally, it returns the optimal policy matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IryDdEYmDs8Y"
   },
   "outputs": [],
   "source": [
    "def policy_iteration_algorithm(environment, discount_factor=1.0, max_iterations=1000):\n",
    "    num_states = environment.observation_space.n\n",
    "    num_actions = environment.action_space.n\n",
    "    policy_matrix = np.ones(shape=[num_states, num_actions]) / num_actions\n",
    "    evaluated_policies_count = 1\n",
    "\n",
    "    for iteration in range(int(max_iterations)):\n",
    "        stable_policy = False\n",
    "        value_function = evaluate_policy(policy_matrix, environment, discount_factor)\n",
    "\n",
    "        for current_state in range(num_states):\n",
    "            current_action = np.argmax(policy_matrix[current_state])\n",
    "            action_values = calculate_state_value(environment, current_state, value_function, discount_factor)\n",
    "            best_action = np.argmax(action_values)\n",
    "\n",
    "            if current_action != best_action:\n",
    "                stable_policy = True\n",
    "\n",
    "            policy_matrix[current_state] = np.eye(num_actions)[best_action]\n",
    "\n",
    "        evaluated_policies_count += 1\n",
    "\n",
    "        if stable_policy:\n",
    "            print(f'Found a stable policy after {evaluated_policies_count:,} evaluations.\\n')\n",
    "            return policy_matrix, value_function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finds the optimal value function for a Markov Decision Process (MDP) iteratively.\n",
    "\n",
    "Computes the optimal value function directly instead of improving a policy iteratively.\n",
    "\n",
    "Initializes state values to zeros.\n",
    "\n",
    "Iteratively updates state values until convergence or reaching the maximum number of iterations.\n",
    "\n",
    "For each state, computes action values and selects the best action value.\n",
    "\n",
    "Updates state values based on the best action value and tracks the maximum change in state values (delta).\n",
    "\n",
    "Terminates when the maximum change in state values falls below the convergence threshold.\n",
    "\n",
    "Extracts the optimal policy by selecting the action with the highest value for each state.\n",
    "\n",
    "Policy matrix indicates the probability of selecting each action in each state.\n",
    "\n",
    "Efficient for finding optimal policy in finite MDPs by computing the optimal value function directly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v2-KjjYmyKSM"
   },
   "outputs": [],
   "source": [
    "def value_iteration_algorithm(environment, discount_factor=1e-1, convergence_threshold=1e-9, max_iterations=1e4):\n",
    "    state_values = np.zeros(environment.observation_space.n)\n",
    "\n",
    "    for iteration in range(int(max_iterations)):\n",
    "        delta = 0\n",
    "\n",
    "        for current_state in range(environment.observation_space.n):\n",
    "            action_values = calculate_state_value(environment, current_state, state_values, discount_factor)\n",
    "            best_action_value = np.max(action_values)\n",
    "            delta = max(delta, np.abs(state_values[current_state] - best_action_value))\n",
    "            state_values[current_state] = best_action_value\n",
    "\n",
    "        if delta < convergence_threshold:\n",
    "            print(f'\\nValue iteration converged at iteration #{iteration+1:,}')\n",
    "            break\n",
    "\n",
    "    policy_matrix = np.zeros(shape=[environment.observation_space.n, environment.action_space.n])\n",
    "\n",
    "    for current_state in range(environment.observation_space.n):\n",
    "        action_values = calculate_state_value(environment, current_state, state_values, discount_factor)\n",
    "        best_action = np.argmax(action_values)\n",
    "        policy_matrix[current_state, best_action] = 1.0\n",
    "\n",
    "    return policy_matrix, state_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing Episodes and Evaluating Policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is useful for evaluating the performance of a given policy matrix in the Frozen Lake environment by simulating gameplay over multiple episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HA0k2Qh8yK7R"
   },
   "outputs": [],
   "source": [
    "def play_episodes_and_evaluate(env, num_episodes, policy_matrix, max_actions=100, render=False):\n",
    "\n",
    "    total_wins = 0\n",
    "    total_rewards, total_actions = 0, 0\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        current_state = env.reset()\n",
    "        episode_done, actions_taken = False, 0\n",
    "\n",
    "        while actions_taken < max_actions:\n",
    "            selected_action = np.argmax(policy_matrix[current_state])\n",
    "            next_state, reward, episode_done, _ = env.step(selected_action)\n",
    "\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            actions_taken += 1\n",
    "            total_rewards += reward\n",
    "            current_state = next_state\n",
    "\n",
    "            if episode_done:\n",
    "                total_wins += 1\n",
    "                break\n",
    "\n",
    "        total_actions += actions_taken\n",
    "\n",
    "    print(f'Total rewards: {total_rewards:,}\\tMax actions: {actions_taken:,}')\n",
    "\n",
    "    average_reward = total_rewards / num_episodes\n",
    "    average_actions = total_actions / num_episodes\n",
    "\n",
    "    print('')\n",
    "    return total_wins, total_rewards, average_reward, average_actions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function serves as a comprehensive evaluation tool for comparing the performance of different iteration methods in the Frozen Lake environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oo2MuWYQFnAW"
   },
   "outputs": [],
   "source": [
    "num_episodes = 1000\n",
    "\n",
    "def agent_and_evaluate(env):\n",
    "\n",
    "    total_rewards_list = []\n",
    "\n",
    "    action_mapping = {\n",
    "        0: '\\u2191',  # up\n",
    "        1: '\\u2192',  # right\n",
    "        2: '\\u2193',  # down\n",
    "        3: '\\u2190'   # left\n",
    "    }\n",
    "\n",
    "    iteration_methods = [\n",
    "        ('Policy Iteration', policy_iteration_algorithm),\n",
    "        ('Value Iteration', value_iteration_algorithm)\n",
    "    ]\n",
    "\n",
    "    for method_name, method_func in iteration_methods:\n",
    "        policy_matrix, value_function = method_func(env)\n",
    "\n",
    "        print(f'Final policy using {method_name}:')\n",
    "        print(' '.join([action_mapping[action] for action in np.argmax(policy_matrix, axis=1)]))\n",
    "\n",
    "        total_wins, total_rewards, avg_reward, avg_actions = play_episodes_and_evaluate(env, num_episodes, policy_matrix)\n",
    "        total_rewards_list.append(total_rewards)\n",
    "\n",
    "        print(f'Number of wins = {total_wins:,}')\n",
    "        print(f'Average reward = {avg_reward:.2f}')\n",
    "        print(f'Average actions = {avg_actions:.2f}')\n",
    "\n",
    "    return total_rewards_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g2UG0B9OGdjA",
    "outputId": "8eff5873-bad4-461d-bc8e-88b0fe5c1283"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy evaluation terminated after 66 iterations.\n",
      "\n",
      "Found a stable policy after 2 evaluations.\n",
      "\n",
      "Final policy using Policy Iteration:\n",
      "↑ ← ↑ ← ↑ ↑ ↑ ↑ ← → ↑ ↑ ↑ ↓ → ↑\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rewards: 735.0\tMax actions: 45\n",
      "\n",
      "Number of wins = 1,000\n",
      "Average reward = 0.73\n",
      "Average actions = 41.14\n",
      "\n",
      "Value iteration converged at iteration #8\n",
      "Final policy using Value Iteration:\n",
      "→ ← ↓ ← ↑ ↑ ↑ ↑ ← → ↑ ↑ ↑ ↓ → ↑\n",
      "Total rewards: 438.0\tMax actions: 24\n",
      "\n",
      "Number of wins = 1,000\n",
      "Average reward = 0.44\n",
      "Average actions = 27.64\n"
     ]
    }
   ],
   "source": [
    "rewards = agent_and_evaluate(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration: and Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy Iteration:\n",
    "\n",
    "Convergence: Policy evaluation converged after 66 iterations.\n",
    "Stability: A stable policy was found after 2 evaluations.\n",
    "Final Policy: Represents recommended actions for each state.\n",
    "Total Rewards: Achieved 735.0 cumulative rewards across all episodes.\n",
    "Max Actions: 45 actions in the longest episode.\n",
    "Wins: All 1,000 episodes resulted in wins.\n",
    "Average Reward: 0.73 per episode.\n",
    "Average Actions: 41.14 actions per episode.\n",
    "Value Iteration:\n",
    "\n",
    "Convergence: Value iteration converged after 8 iterations.\n",
    "Final Policy: Similar representation as Policy Iteration.\n",
    "Total Rewards: Achieved 438.0 cumulative rewards.\n",
    "Max Actions: 24 actions in the longest episode.\n",
    "Wins: All 1,000 episodes resulted in wins.\n",
    "Average Reward: 0.44 per episode.\n",
    "Average Actions: 27.64 actions per episode.\n",
    "Comparison:\n",
    "\n",
    "Average Reward: Policy Iteration outperformed with 0.73 compared to 0.44.\n",
    "Average Actions: Value Iteration was more efficient with 27.64 compared to 41.14.\n",
    "Conclusion:\n",
    "\n",
    "Policy Iteration offers higher rewards per episode.\n",
    "Value Iteration requires fewer actions, potentially finding more efficient paths.\n",
    "Selection depends on specific goals and requirements.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
